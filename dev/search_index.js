var documenterSearchIndex = {"docs":
[{"location":"store/#Store","page":"Data Store","title":"Store","text":"As many experiments may require a data of data to be preloaded for each trial, Experiment.jl provides a data store that can be initialised once on each worker to reduce the amount of time required for loading the same data.\n\nThis store is intended as a read-only store that is reused upon execution of each of the trials. ","category":"section"},{"location":"store/#Usage","page":"Data Store","title":"Usage","text":"To start, you must create a function which creates the data to be stored, similar to the functions that runs the trial. As en example:\n\n# Goes inside the same file as your experiment run file (i.e. the file that get's included).\nfunction create_global_store(config)\n    # config is the global configuration given to the experiment\n    data = Dict{Symbol, Any}(\n        :dataset => rand(1000),\n        :flag => false,\n        # etc...\n    )\n    return data\nend\n\nThe variable config will be the configuration provided to the Experiment struct created for your experiment. Importantly, this function will return a Dict{Symbol, Any}.\n\nThe name of this function can be anything, but you need to supply it to the experiment when it is being created, i.e.\n\nexperiment = Experiment(\n    name=\"Test Experiment\",\n    include_file=\"run.jl\",\n    function_name=\"run_trial\",\n    init_store_function_name=\"create_global_store\",\n    configuration=config\n)\n\nInside your run_trial function, you can access the global store using get_global_store\n\nusing Experimenter # exports get_global_store\nfunction run_trial(config, trial_id)\n    store = get_global_store()\n    dataset = store[:dataset]\n    # gather your results\n    return results\nend","category":"section"},{"location":"api/#Public-API","page":"Public API","title":"Public API","text":"","category":"section"},{"location":"api/#Database-Management","page":"Public API","title":"Database Management","text":"","category":"section"},{"location":"api/#Experiments","page":"Public API","title":"Experiments","text":"","category":"section"},{"location":"api/#Data-Storage","page":"Public API","title":"Data Storage","text":"","category":"section"},{"location":"api/#Trials","page":"Public API","title":"Trials","text":"","category":"section"},{"location":"api/#Execution","page":"Public API","title":"Execution","text":"","category":"section"},{"location":"api/#Cluster-Management","page":"Public API","title":"Cluster Management","text":"","category":"section"},{"location":"api/#Snapshots","page":"Public API","title":"Snapshots","text":"","category":"section"},{"location":"api/#Misc","page":"Public API","title":"Misc","text":"","category":"section"},{"location":"api/#Experimenter.open_db","page":"Public API","title":"Experimenter.open_db","text":"open_db(database_name, [experiment_folder, create_folder]; in_memory=false)\n\nOpens a database and prepares it with the Experimenter.jl schema with tables for Experiment, Trial and Snapshot. If the database already exists, it will open it and not overwrite the existing data.\n\nSetting in_memory to true will skip all of the arguments and create the database \"in memory\" and hence, will not persist.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.export_db","page":"Public API","title":"Experimenter.export_db","text":"export_db(db::ExperimentDatabase, outfile::AbstractString, experiment_names...)\n\nOpens a new database at outfile and inserts experiments from db into the new db, where the names of the experiment are listed in the final input.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.restore_from_db","page":"Public API","title":"Experimenter.restore_from_db","text":"restore_from_db(db::ExperimentDatabase, experiment::Experiment)\n\nSearches the db for the supplied experiment, matching on the configuration and the name, disregarding the unique ID.\n\nIf experiment already exists in the db, returns that experiment with the db's UUID for it, otherwise return the input experiment.\n\nWill error if the experiment exists but does not match the input experiment configuration.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.merge_databases!","page":"Public API","title":"Experimenter.merge_databases!","text":"merge_databases!(primary_db, secondary_db)\n\nSearches all of the records from the secondary database and adds them to the first database.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.Experiment","page":"Public API","title":"Experimenter.Experiment","text":"Experiment\n\nA database object for storing the configuration options of an experiment.\n\nThe signature of the function supplied should be:\n\nfn(configuration::Dict{Symbol, Any}, trial_id::UUID)\n\nThe function should be available when including the file provided.\n\nA name is required to uniquely label this experiment.\n\n\n\n\n\n","category":"type"},{"location":"api/#Experimenter.get_progress","page":"Public API","title":"Experimenter.get_progress","text":"get_progress(db::ExperimentDatabase, name)\n\nReturns a table of the trials of an experiment, identified by the name parameter. Returns details of the progress and configuration, but not the results.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.get_experiment","page":"Public API","title":"Experimenter.get_experiment","text":"get_experiment(db::ExperimentDatabase, experiment_id)\n\nSearches the db for the given experiment_id which can be given as a string or UUID.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.get_experiments","page":"Public API","title":"Experimenter.get_experiments","text":"get_experiments(db::ExperimentDatabase)\n\nReturns a vector of all experiments in the database.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.get_experiment_by_name","page":"Public API","title":"Experimenter.get_experiment_by_name","text":"get_experiment(db::ExperimentDatabase, name)\n\nSearches the db for an experiment with the experiment name set to name. Returns that experiment.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.get_ratio_completed_trials_by_name","page":"Public API","title":"Experimenter.get_ratio_completed_trials_by_name","text":"get_ratio_completed_trials_by_name(db::ExperimentDatabase, name)\n\nCalculates the ratio of completed trials for the given experiment with name name, without fetching the results.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.get_global_store","page":"Public API","title":"Experimenter.get_global_store","text":"get_global_store()\n\nTries to get the global store that is initialised by the supplied function with the name specified by init_store_function_name set in  the running experiment. This store is local to each worker.\n\nSetup\n\nTo create the store, add a function in your include file which returns a dictionary of type Dict{Symbol, Any}, which has the signature similar to:\n\nfunction create_global_store(config)\n    # config is the global configuration given to the experiment\n    data = Dict{Symbol, Any}(\n        :dataset => rand(1000),\n        :flag => false,\n        # etc...\n    )\n    return data\nend\n\nInside your main experiment execution function, you can get this store via get_global_store, which is exported by Experimenter.\n\nfunction myrunner(config, trial_id)\n    store = get_global_store()\n    dataset = store[:dataset] # Retrieve the keys from the store\n    # process data\n    return results\nend\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.get_results_from_trial_global_database","page":"Public API","title":"Experimenter.get_results_from_trial_global_database","text":"get_results_from_trial_global_database(trial_id::UUID)\n\nGets the results of a specific trial from the global database. Redirects to the master node if on a worker node. Locks to secure access.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.get_trial","page":"Public API","title":"Experimenter.get_trial","text":"get_trial(db::ExperimentDatabase, trial_id)\n\nGets the trial with the matching trial_id (string or UUID) from the database.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.get_trials","page":"Public API","title":"Experimenter.get_trials","text":"get_trial(db::ExperimentDatabase, experiment_id)\n\nGets all trials from the database under the experiment_id supplied.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.get_trials_by_name","page":"Public API","title":"Experimenter.get_trials_by_name","text":"get_trials_by_name(db::ExperimentDatabase, name)\n\nGets all trials from the database for the experiment with the name name.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.get_trials_ids_by_name","page":"Public API","title":"Experimenter.get_trials_ids_by_name","text":"get_trials_ids_by_name(db::ExperimentDatabase, name)\n\nGets just the trial IDs from the database for the experiment with the name name.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.@execute","page":"Public API","title":"Experimenter.@execute","text":"@execute experiment database [mode=SerialMode use_progress=false directory=pwd()]\n\nRuns the experiment out of global scope, saving results in the database, skipping all already executed trials.\n\nArgs:\n\nmode: Specifies SerialMode, MultithreadedMode or DistributedMode to execute serially or in parallel. use_progress: Shows a progress bar directory: Directory to change the current process (or worker processes) to for execution.\n\n\n\n\n\n","category":"macro"},{"location":"api/#Experimenter.ExecutionModes.SerialMode","page":"Public API","title":"Experimenter.ExecutionModes.SerialMode","text":"Executes the trials of the experiment one of the other, sequentially.\n\n\n\n\n\n","category":"type"},{"location":"api/#Experimenter.ExecutionModes.MultithreadedMode","page":"Public API","title":"Experimenter.ExecutionModes.MultithreadedMode","text":"Executes the trials of the experiment in parallel using Threads.@Threads\n\n\n\n\n\n","category":"type"},{"location":"api/#Experimenter.ExecutionModes.DistributedMode","page":"Public API","title":"Experimenter.ExecutionModes.DistributedMode","text":"Executes the trials of the experiment in parallel using Distributed.jls pmap.\n\n\n\n\n\n","category":"type"},{"location":"api/#Experimenter.ExecutionModes.HeterogeneousMode","page":"Public API","title":"Experimenter.ExecutionModes.HeterogeneousMode","text":"Executes the trials of the experiment in parallel using a custom scheduler that uses all threads of each worker.\n\n\n\n\n\n","category":"type"},{"location":"api/#Experimenter.ExecutionModes.MPIMode","page":"Public API","title":"Experimenter.ExecutionModes.MPIMode","text":"Executes the trials of the experiment in parallel using MPI, which uses one MPI node for coordination and saving of jobs.\n\n\n\n\n\n","category":"type"},{"location":"api/#Experimenter.Cluster.init","page":"Public API","title":"Experimenter.Cluster.init","text":"init(; kwargs...)\n\nChecks the environment variables to see if a script is running on a cluster  and then launches the processes as determined by the environment variables.\n\nArguments\n\nThe keyword arguments are forwarded to the init function for each cluster management system. Check the ext folder for extensions to see which keywords are supported.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.get_snapshots","page":"Public API","title":"Experimenter.get_snapshots","text":"get_snapshots(db::ExperimentDatabase, trial_id)\n\nGets all the associated snapshots (as a vector) from the database for a given trial with matching trial_id.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.latest_snapshot","page":"Public API","title":"Experimenter.latest_snapshot","text":"latest_snapshot(db::ExperimentDatabase, trial_id)\n\nGets the latest snapshot from the database for a given trial with matching trial_id, using the date of the most recent snapshot.\n\nKnown to have issues when snapshots are created within the same second.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.save_snapshot!","page":"Public API","title":"Experimenter.save_snapshot!","text":"save_snapshot!(db::ExperimentDatabase, trial_id::UUID, state::Dict{Symbol,Any}, [label])\n\nSaves the snapshot with given state in the database, associating with the trial with matching trial_id. Automatically saves the time of the snapshot.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.get_latest_snapshot_from_global_database","page":"Public API","title":"Experimenter.get_latest_snapshot_from_global_database","text":"get_latest_snapshot_from_global_database(trial_id::UUID)\n\nSame as get_latest_snapshot, but in the given global database. Redirects to the master worker if on a distributed node. Only works when using @execute.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.save_snapshot_in_global_database","page":"Public API","title":"Experimenter.save_snapshot_in_global_database","text":"save_snapshot_in_global_database(trial_id::UUID, state, [label])\n\nSave the results of a specific trial from the global database, with the supplied state and optional label. Redirects to the master node if on a worker node. Locks to secure access.\n\n\n\n\n\n","category":"function"},{"location":"api/#Experimenter.LinearVariable","page":"Public API","title":"Experimenter.LinearVariable","text":"LinearVariable(min, max, n)\n\nSpecifies a range for a parameter variable to take, from min to max inclusive with n total values.\n\n\n\n\n\n","category":"type"},{"location":"api/#Experimenter.LogLinearVariable","page":"Public API","title":"Experimenter.LogLinearVariable","text":"LogLinearVariable(min, max, n)\n\nA linearly spaced parameter variable in log space. If min=1 and max=100 and n=3 then the values are [1,10,100].\n\n\n\n\n\n","category":"type"},{"location":"api/#Experimenter.RepeatVariable","page":"Public API","title":"Experimenter.RepeatVariable","text":"RepeatVariable(val, n)\n\nSpecifies a parameter variable that outputs the same value val n times. \n\n\n\n\n\n","category":"type"},{"location":"api/#Experimenter.IterableVariable","page":"Public API","title":"Experimenter.IterableVariable","text":"IterableVariable(iter)\n\nWraps a given iterator iter to tell the experiment to perform a grid search over each element of the iterator for the given parameter.\n\n\n\n\n\n","category":"type"},{"location":"api/#Experimenter.MatchIterableVariable","page":"Public API","title":"Experimenter.MatchIterableVariable","text":"MatchIterableVariable(iter)\n\nThis type of variable matches with the product from the other AbstractVariables in the configuration.\n\nThis does not form part of the product variables (grid search), but instead uniques matches with that product.\n\n\n\n\n\n","category":"type"},{"location":"snapshots/#Custom-Snapshots","page":"Custom Snapshots","title":"Custom Snapshots","text":"Most simulated experiments take a long time to run, and may be cancelled part way. It is important to be able to save progress on these long-running simulations. For this example, we will take the idea of simulating a Monte-Carlo process. Imagine that your process looks like the below function: \n\nusing Random\nfunction run_simulation(config::Dict{Symbol, Any}, trial_id)\n    epochs = config[:epochs]\n    T = Float64\n    positions = zeros(T, epochs)\n    x = zero(T)\n    for t in 2:epochs\n        x += randn(T)\n        positions[t] = x\n    end\n\n    results = Dict{Symbol, Any}(\n        mean_position => sum(positions) / length(positions)\n    )\n    return results\nend\n\nIf we want to be able to replicate this process, we should take in a seed for the random values and save a snapshot every so often:\n\nusing Random\nusing Experimenter\nfunction run_simulation(config::Dict{Symbol, Any}, trial_id)\n    epochs = config[:epochs]\n    seed = config[:seed]\n    snapshot_interval = config[:snapshot_interval]\n    snapshot_label = config[:snapshot_label]\n\n    rng = Random.Xoshiro(seed)\n    T = Float64\n    positions = zeros(T, epochs)\n    x = zero(T)\n    for t in 2:epochs\n        x += randn(rng, T)\n        positions[t] = x\n        if t % snapshot_interval == 0\n            state = Dict{Symbol, Any}(\n                :rng_state => copy(rng),\n                :positions => positions[begin:t]\n            )\n            # Global will only work when executing globally with @execute!\n            save_snapshot_in_global_database(trial_id, state, snapshot_label)\n        end\n    end\n\n    results = Dict{Symbol, Any}(\n        :mean_position => sum(positions) / length(positions)\n    )\n    return results\nend\n\nThis will save a snapshot associated with the trial_id supplied, whose key is based on the current time. So far, we have only saved the snapshot, but we should implement a method which initialises our simulation, loading from snapshot:\n\nusing Logging\nfunction init_sim(config::Dict{Symbol,Any}, trial_id)\n    snapshot = get_latest_snapshot_from_global_database(trial_id)\n\n    rng = Random.Xoshiro(config[:seed])\n    T = Float64\n    positions = zeros(T, config[:epochs])\n    x = zero(T)\n    start_t = 2\n    if !isnothing(snapshot)\n        state = snapshot.state # Dict we saved earlier\n        copy!(rng, state[:rng_state]) # Reset RNG\n        saved_positions = state[:positions]\n        # Load existing positions\n        positions[begin:length(saved_positions)] .= saved_positions\n        x = last(saved_positions)\n        start_t = length(saved_positions) + 1\n        @info \"Restored trial $trial_id from snapshot - $(length(saved_positions)) epochs restored.\"\n    end\n\n    return x, positions, start_t, rng, T\nend\n\nFinally, we put it altogether:\n\n# saved in `run.jl`\nusing Random\nusing Experimenter\nfunction run_simulation(config::Dict{Symbol, Any}, trial_id)\n    epochs = config[:epochs]\n    snapshot_interval = config[:snapshot_interval]\n    snapshot_label = config[:snapshot_label]\n\n    x, positions, start_t, rng, T = init_sim(config, trial_id)\n\n    for t in start_t:epochs\n        x += randn(rng, T)\n        positions[t] = x\n\n        if t % snapshot_interval == 0\n            state = Dict{Symbol, Any}(\n                :rng_state => copy(rng),\n                :positions => positions[begin:t]\n            )\n            # Global will only work when executing globally with @execute!\n            save_snapshot_in_global_database(trial_id, state, snapshot_label)\n        end\n    end\n\n    results = Dict{Symbol, Any}(\n        :mean_position => sum(positions) / length(positions)\n    )\n    return results\nend\n# ... include definiton for init_sim function.\n\nNow we can create a script to execute this project:\n\nusing Experimenter\n\nconfig = Dict{Symbol, Any}(\n    :seed => IterableVariable([1234,4567,8910]),\n    :epochs => IterableVariable([500_000, 1_000_000]),\n    :snapshot_interval => 100_000,\n    :snapshot_label => \"MC Snapshots\"\n)\nexperiment = Experiment(\n    name=\"Snapshot Experiment\",\n    include_file=\"run.jl\",\n    function_name=\"run_simulation\",\n    configuration=deepcopy(config)\n)\ndb = open_db(\"experiments.db\")\n@execute experiment db SerialMode true\n\nYou can use Ctrl+C to cancel the execution before it is complete, and run again to see if the logger has been triggered (i.e. a snapshot has been loaded). If the program runs too quickly, try adding a sleep(0.1) whenever a snapshot is saved, so you get a chance to cancel it to see if it works.","category":"section"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"Experimenter.jl is a package that is designed to help you keep track of your experiments and their results. It is built to work with Distributed.jl for parallel writing of results to a SQLite database file.","category":"section"},{"location":"getting_started/#Opening-the-database","page":"Getting Started","title":"Opening the database","text":"To get started, first import the library with:\n\nusing Experimenter\n\nAfter this one needs to create a database to store the results:\n\ndb = open_db(\"experiments.db\")\n\nOne can always supply a given directory for the database as well, for example:\n\ndb = open_db(\"experiments.db\", joinpath(pwd(), \"results\"))\n\nThe first call to open_db will check if a file already exists. If the file does not exist, Experimenter.jl will create the file and the schema for the database.","category":"section"},{"location":"getting_started/#Defining-the-work-we-want-to-process","page":"Getting Started","title":"Defining the work we want to process","text":"To run an experiment we need to first define a function which runs our experiment:\n\n# in run.jl\nusing Random\nfunction run_trial(config::Dict{Symbol,Any}, trial_id)\n    results = Dict{Symbol, Any}()\n    sigma = config[:sigma]\n    N = config[:N]\n    seed = config[:seed]\n    rng = Random.Xoshiro(seed)\n    # Perform some calculation\n    results[:distance] = sum(rand(rng) * sigma for _ in 1:N)\n    # Must return a Dict{Symbol, Any}, with the data we want to save\n    return results\nend","category":"section"},{"location":"getting_started/#Creating-an-experiment","page":"Getting Started","title":"Creating an experiment","text":"Now we can define a configuration for our experiment:\n\n# in a script\nconfig = Dict{Symbol,Any}(\n    :N => IterableVariable([10, 20]),\n    :seed => IterableVariable([1234, 4321]),\n    :sigma => 1.0\n)\n\nThis is just a dictionary, with some special wrappers IterableVariable for some of the config values. When we create our experiment, we pass in this configuration and the path to the file with the function to run our experiment:\n\nexperiment = Experiment(\n    name=\"Test Experiment\",\n    include_file=\"run.jl\",\n    function_name=\"run_trial\",\n    configuration=deepcopy(config)\n)","category":"section"},{"location":"getting_started/#Examining-the-trials-of-an-experiment","page":"Getting Started","title":"Examining the trials of an experiment","text":"We can look at the set of trials this experiment will create:\n\nfor trial in experiment\n    println(trial.configuration)\nend\n# Dict{Symbol, Any}(:N => 10, :sigma => 1.0, :seed => 1234)\n# Dict{Symbol, Any}(:N => 20, :sigma => 1.0, :seed => 1234)\n# Dict{Symbol, Any}(:N => 10, :sigma => 1.0, :seed => 4321)\n# Dict{Symbol, Any}(:N => 20, :sigma => 1.0, :seed => 4321)\n\nor, alternatively:\n\ntrials = collect(experiment)\n\nThere are multiple trials in this experiment as we used an IterableVariable wrapper, which says that we want to run a grid search over these specific variables.","category":"section"},{"location":"getting_started/#Executing-an-experiment","page":"Getting Started","title":"Executing an experiment","text":"To execute our experiment, we use the @execute macro. To execute the experiment serially:\n\n@execute experiment db SerialMode\n\nInstead of SerialMode, we can use ThreadedMode to execute via Threads.@threads, or use DistributedMode to execute via a pmap and run across different workers.","category":"section"},{"location":"getting_started/#Getting-the-results","page":"Getting Started","title":"Getting the results","text":"Once the experiments are completed, we can run:\n\ntrials = get_trials_by_name(db, \"Test Experiment\");\n\nThis will return a Vector{Trial}, where Trial has a results field which is the dictionary we returned from the run_trial function. To get the results we write:\n\nresults = [t.results for t in trials]","category":"section"},{"location":"getting_started/#Re-running-failed-trials","page":"Getting Started","title":"Re-running failed trials","text":"If a trial did not finish, then the results field will be missing. Whenever we run the @execute macro, it will skip any trial that already has results, and only run the next trials. Therefore \n\n@execute experiment db SerialMode\n\nwill not run any more trials, as they have already been completed. However, if the execution stopped (for example killed by the SLURM scheduler due to wall time), then it will only run the trials that have not been completed.","category":"section"},{"location":"getting_started/#Saving-part-way","page":"Getting Started","title":"Saving part way","text":"If your trials take a long time to finish and may be cancelled during their run, you can always implement a way to save a Snapshot, which allows you to save data you need to restore a trial part way through running. An example setup for doing this is given in Custom Snapshots.","category":"section"},{"location":"getting_started/#What-is-an-Experiment?","page":"Getting Started","title":"What is an Experiment?","text":"An experiment sets up a configuration that specifies (by default) a grid search over variables. If none of the special classes such as IterableVariable, LinearVariable, LogLinearVariable etc are used as values in the configuration dictionary, this will specify only a single trial. However, if these special types are used, an experiment will have multiple trials, whose configurations are created via a grid search over the special AbstractVariables provided, with each of these values being replaced by a single element in these iterables.\n\nAs an example the following configuration:\n\nconfig = Dict{Symbol, Any}(\n    :a => IterableVariable([\"a\", \"b\"]),\n    :b => LinearVariable(1, 4, 5),\n    :c => \"constant value\"\n)\n\nSince the first two parts are marked with a type of AbstractVariable (or concrete type of), these will form our grid search. The actual configurations will look like the following code:\n\nfor a in [\"a\", \"b\"]\n    for b in LinRange(1, 4, 5)\n        trial_config = Dict{Symbol, Any}(\n            :a => a,\n            :b => b,\n            :c => \"constant value\"\n        )\n    end\nend\n\nA matched variable will not form part of the grid, but works as follows:\n\nmatched = rand(2*5)\ni = 1\nfor a in [\"a\", \"b\"]\n    for b in LinRange(1, 4, 5)\n        trial_config = Dict{Symbol, Any}(\n            :a => a,\n            :b => b,\n            :c => \"constant value\",\n            :matched => matched[i]\n        )\n        i += 1\n    end\nend\n\nThe matched variable must have as many entries as there are in the grid search.","category":"section"},{"location":"clusters/#Cluster-Execution","page":"Cluster Execution","title":"Cluster Execution","text":"This package is most useful for running grid search trials on a cluster environment (i.e. a HPC), or a single node with many CPUs. \n\nThere are two main ways you can distribute your experiment over many processes - DistributedMode or MPIMode. \n\nFor those using a distributed cluster, we recommend that you launch your jobs using the MPI functionality, instead of the legacy SLURM support (see the SLURM section below for details).","category":"section"},{"location":"clusters/#MPI","page":"Cluster Execution","title":"MPI","text":"","category":"section"},{"location":"clusters/#Installation","page":"Cluster Execution","title":"Installation","text":"Most HPC environments have access to their own MPI implementation. These MPI implementations often take advantage of proprietary interconnect (networking) between the nodes that allow for low-latency and high-throughput communication. If you would like to find your local HPC's implementation, you may be able to look through the catalogue via a bash terminal, using the Environment Modules package available on most HPC systems:\n\nmodule avail\n\nor, for a more directed search:\n\nmodule spider mpi\n\nYou may have multiple versions. If you are unsure as to which version to use, check the documentation for the HPC, contact your local System Administrator or simply use what is available. Using OpenMPI is often a reliable choice. \n\nYou can load which version of MPI you would like by adding\n\nmodule load mpi/latest\n\nto your job script (remember to change mpi/latest to the package available on your system).\n\nMake you have loaded the MPI version you wish to use by running the module load ... command in the same terminal before opening Julia in the terminal by using\n\njulia --project\n\nRun this command in the same directory as your project.\n\nNow, you have to add the MPI package to your local environment using\n\nimport Pkg; Pkg.add(\"MPI\")\n\nNow you should be able to load MPIPreferences and tell MPI about using your system binary:\n\nusing MPI.MPIPreferences\n\nMPIPreferences.use_system_binary()\nexit()\n\nThis should create a new LocalPreferences.toml file. I would recommend adding this file to your .gitignore list and not committing it to your GitHub repository.","category":"section"},{"location":"clusters/#Job-Scripts","page":"Cluster Execution","title":"Job Scripts","text":"When you are running on a cluster, write your job script so that you load MPI and precompile Julia before launching your job. An example job script could look like the following:\n\n#!/bin/bash\n\n#SBATCH --ntasks=8\n#SBATCH --cpus-per-task=4\n#SBATCH --mem-per-cpu=2048\n#SBATCH --time=00:30:00\n#SBATCH -o mpi_job_%j.out\n\n\nmodule load mpi/latest\nmodule load julia/1.10.2\n\n# Precompile Julia first to avoid race conditions\njulia --project --threads=4 -e 'import Pkg; Pkg.instantiate()'\njulia --project --threads=4 -e 'import Pkg; Pkg.precompile()'\n\nmpirun -n 8 julia --project --threads=4 my_experiment.jl\n\nUse the above as a template and change the specifics to suit your specific workload and HPC.\n\ninfo: Info\nMake sure that you launch your jobs with at least 2 processes (tasks), as one task is dedicated towards coordinating the execution of trials and saving the results.","category":"section"},{"location":"clusters/#Experiment-file","page":"Cluster Execution","title":"Experiment file","text":"As usual, you should write a script to define your experiment and run the configuration. Below is an example, where it is assumed there is another file called run.jl which contains a function run_trial which takes a configuration dictionary and a trial UUID.\n\nusing Experimenter\n\nconfig = Dict{Symbol,Any}(\n    :N => IterableVariable([Int(1e6), Int(2e6), Int(3e6)]),\n    :seed => IterableVariable([1234, 4321, 3467, 134234, 121]),\n    :sigma => 0.0001)\nexperiment = Experiment(\n    name=\"Test Experiment\",\n    include_file=\"run.jl\",\n    function_name=\"run_trial\",\n    configuration=deepcopy(config)\n)\n\ndb = open_db(\"experiments.db\")\n\n# Init the cluster\nExperimenter.Cluster.init()\n\n@execute experiment db MPIMode(1)\n\nNote that we are calling MPIMode(1) which says that we want a communication batch size of 1. If your jobs are small, and you want each worker to process a batch at a time, you can set this to a higher number.","category":"section"},{"location":"clusters/#SLURM","page":"Cluster Execution","title":"SLURM","text":"warning: Warning\nIt is recommended that you use the above MPI mode to run jobs on a cluster, instead of relying on ClusterManagers.jl, as it is much slower to run jobs.\n\nNormally when running on SLURM, one creates a bash script to tell the scheduler about the resource requirements for a job. The following is an example:\n\n#!/bin/bash\n\n#SBATCH --nodes=2\n#SBATCH --ntasks=2\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=1024\n#SBATCH --time=00:30:00\n#SBATCH -o hpc/output/test_job_%j.out\n\nThe function [Experimenter.Cluster.create_slurm_template] provides an easy way to create one of these bash scripts with everything you need to run.","category":"section"},{"location":"clusters/#Example","page":"Cluster Execution","title":"Example","text":"Let us take the following end-to-end example. Say that we have an experiment script at my_experiment.jl (contents below), which now initialises the cluster:\n\nusing Experimenter\n\nconfig = Dict{Symbol,Any}(\n    :N => IterableVariable([Int(1e6), Int(2e6), Int(3e6)]),\n    :seed => IterableVariable([1234, 4321, 3467, 134234, 121]),\n    :sigma => 0.0001)\nexperiment = Experiment(\n    name=\"Test Experiment\",\n    include_file=\"run.jl\",\n    function_name=\"run_trial\",\n    configuration=deepcopy(config)\n)\n\ndb = open_db(\"experiments.db\")\n\n# Init the cluster\nExperimenter.Cluster.init()\n\n@execute experiment db DistributedMode\n\nAdditionally, we have the file run.jl containing:\n\nusing Random\nusing Distributed\nfunction run_trial(config::Dict{Symbol,Any}, trial_id)\n    results = Dict{Symbol, Any}()\n    sigma = config[:sigma]\n    N = config[:N]\n    seed = config[:seed]\n    rng = Random.Xoshiro(seed)\n    # Perform some calculation\n    results[:distance] = sum(rand(rng) * sigma for _ in 1:N)\n    results[:num_threads] = Threads.nthreads()\n    results[:hostname] = gethostname()\n    results[:pid] = Distributed.myid()\n    # Must return a Dict{Symbol, Any}, with the data we want to save\n    return results\nend\n\nWe can now create a bash script to run our experiment. We create a template by running the following in the terminal (or adjust or the REPL)\n\njulia --project -e 'using Experimenter; Experimenter.Cluster.create_slurm_template(\"myrun.sh\")'\n\nWe then modify the create myrun.sh file to the following:\n\n#!/bin/bash\n\n#SBATCH --ntasks=4\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=1024\n#SBATCH --time=00:30:00\n#SBATCH -o hpc/logs/job_%j.out\n\njulia --project --threads=1 my_experiment.jl\n\n# Optional: Remove the files created by ClusterManagers.jl\nrm -fr julia-*.out\n\n\nOnce written, we execute this on the cluster via\n\nsbatch myrun.sh\n\nWe can then open a Julia REPL (once the job has finished) to see the results:\n\nusing Experimenter\ndb = open_db(\"experiments.db\")\ntrials = get_trials_by_name(db, \"Test Experiment\")\n\nfor (i, t) in enumerate(trials)\n    hostname = t.results[:hostname]\n    id = t.results[:pid]\n    println(\"Trial $i ran on $hostname on worker $id\")\nend\n\nSupport for running on SLURM is based on this gist available on GitHub. This gist also provides information on how to adjust the SLURM script to allow for one GPU to be allocated to each worker.","category":"section"},{"location":"execution/#Running-your-Experiments","page":"Running your Experiments","title":"Running your Experiments","text":"Once you have created an experiment you can run it with the @execute macro supplied by Experimenter.jl, suppose you already have an experiment stored in the experiment variable and a database opened with the variable db, then you can execute simply with:\n\n@execute experiment db SerialMode\n\nWhich will only execute trials from the experiment that have not been completed. It is up to you to implement how to continue your simulations from snapshots, using the Snapshots API. ","category":"section"},{"location":"execution/#Single-Node-Parallel","page":"Running your Experiments","title":"Single Node Parallel","text":"There are two main ways of executing your experiments in parallel: multithreading (Threads) or multiprocessing (Distributed). The former has lower latency, but the latter scales to working on across a cluster. The easiest option if you are executing on a single computer, use:\n\n@execute experiment db MultithreadedMode\n\nBy default, this will use as many threads as you have enabled. You can set this using the environment variable JULIA_NUM_THREADS, or by starting Julia with --threads=X, replacing X with the number you want. You can check what your current setting is with Threads.nthreads().\n\nAlternatively, we can change the execution mode to DistributedMode:\n\n@execute experiment db DistributedMode\n\nThis internally uses pmap from the Distributed.jl standard library, parallelising across all open workers. You can check the number of distributed workers with:\n\nusing Distributed\nnworkers()\n\nExperimenter.jl will not spin up processes for you, this is something you have to do yourself, see Distributed Execution for an in depth example.\n\ninfo: Info\nIf your code has many memory allocations, it may be better to use DistributedMode instead of MultithreadedMode.","category":"section"},{"location":"execution/#Heterogeneous-Execution","page":"Running your Experiments","title":"Heterogeneous Execution","text":"If you want each distributed worker to be able to run multiple jobs at the same time, you can select a heterogeneous execution scheduling mode, which will allow each worker to run multiple trials simultaneously using multithreading. An example use case for this is where you have multiple nodes, each with many cores, and you do not wish to pay the memory cost from each separate process. Additionally, you can load data in a single process which can be reused by each execution in the same process. This mode may also allow multiple trials to share resources, such as a GPU, which typically only supports one process.\n\nTo run this, you simply change the mode to the HeterogeneousMode option, providing the number of threads to use on each worker, e.g.\n\n@execute experiment db HeterogeneousMode(2)\n\nwhich will allow each distributed worker to run two trials simultaneously via multithreading. If this option is selected, it is encouraged that you enable multiple threads per worker when launching the process, e.g. with addprocs:\n\naddprocs(4; exeflags=[\"--threads=2\"])\n\nOtherwise, each worker may only have access to a single thread and the overall performance throughput will be worse.","category":"section"},{"location":"execution/#MPI-Execution","page":"Running your Experiments","title":"MPI Execution","text":"Most HPC clusters use a Message Passing Interface implementation to handle communication between different processes and synchronise tasks. Experimenter.jl now has built-in support for execution via MPI, which has much lower overhead than the built-in Distributed.jl multiprocessing library. See more examples in the Cluster Execution page. ","category":"section"},{"location":"distributed/#Distributed-Execution","page":"Distributed Execution","title":"Distributed Execution","text":"If you want to execute on a single node, but using multiprocessing (i.e. Distributed.jl), then you can start Julia with\n\njulia --project -p 8\n\nTo start Julia with 8 workers. Alternatively, you can add processes while running:\n\nusing Distributed\naddprocs(8)\n\nAs long as nworkers() show more than one worker, then your execution of trials will occur in parallel, across these workers.\n\nOnce the workers have been added, make sure to change your execution mode to DistributedMode to take advantage of the parallelism.\n\nIf you have access to a HPC cluster and would like to use multiple nodes, you can do this easily with Experimenter.jl - see more in Cluster Execution.","category":"section"},{"location":"#Experimenter","page":"Home","title":"Experimenter","text":"A package for easily running experiments for different parameters and saving the results in a centralised database","category":"section"},{"location":"#Package-Features","page":"Home","title":"Package Features","text":"Create a local SQLite database to store the results of your experiment, removing the need to keep track of 1000s of results files for each parameter configuration.\nProvides a standard structure for executing code across a range of parameters.\nProvides saving of results into the database using standard Julia types.\nPromotes writing a script that can be easily committed to a Git repository to keep track of results and parameters used throughout development.\nProvides an @execute macro that will execute an experiment (consisting of many trials with different parameters). Can execute serially, or in parallel with a choice of multithreading or multiprocessing or even MPI mode.\nProvides an easy way to execute trials across a High Performance Cluster (HPC).\nAutomatically skips completed trials, and provides a Snapshots API to allow for partial progress to be saved and reloaded.\n\nHead over to Getting Started to get an overview of this package.","category":"section"},{"location":"#Manual-Outline","page":"Home","title":"Manual Outline","text":"Pages = [\n    \"getting_started.md\",\n    \"execution.md\",\n    \"distributed.md\",\n    \"clusters.md\",\n    \"store.md\",\n    \"snapshots.md\",\n]\nDepth = 2\n\nCheck out the API at Public API.","category":"section"}]
}
